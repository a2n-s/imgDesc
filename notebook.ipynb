{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- The paper\n",
    "### 1. Learning to align visual and language data.\n",
    "Figure 2.  \n",
    "- key insight: \"people make frequent references to some particular, but unknown location in the image.\"\n",
    "- build upon Karpathy et al. [24]: learn to ground dependency tree relations to image regions with a ranking objective.  \n",
    "-> use of bidirectional RNN (BRNN) -> word representations.  \n",
    "-> simplified objective.\n",
    "##### 1.1 representing images.\n",
    "- map images to 20 $h$-dimesional vectors, $\\{v_i | i = 1, ..., 20\\}$.\n",
    "- pre-trained Region CNN (RCNN) on ImageNet + finetuned on the top 200 classes of the ImageNet Detection Challenge.\n",
    "- 19 top regions + 1 for the whole image -> $\\forall i \\in [|1, 20|], v_i = W_m[CNN_{\\theta}(I_{b_i})] + b_m$ (1)\n",
    "  - $W_m$: $h \\times 4096$ dimensional matrix\n",
    "  - $CNN_{\\theta}$: maps bounding boxes pixels to $4096$-dimensional vectors.\n",
    "  - $\\theta$: 60 million parameters\n",
    "##### 1.2 representing sentences: same $h$-dimensional embedding space.\n",
    "- BRNN: 1-hot encoding of N words over an alphabet -> $h$-dimensional vector.\n",
    "```\n",
    "                                                                    +--------------------------------+\n",
    "                                                               .--->| f(e_t + W_b x h_{t+1}^b + b_b) |--> h_t^b --.\n",
    "      +-----------+          +--------------------+           /     +--------------------------------+             \\    +-----------------------------+\n",
    "1_t --| W_m * 1_t |--> x_t --| f(W_e * x_t + b_e) |--> e_t --*                                     (4)              *-->| f(W_d(h_t^f + h_t^b) + b_d) |--> s_t\n",
    "      +-----------+          +--------------------+           \\     +--------------------------------+             /    +-----------------------------+\n",
    "                (2)                             (3)            `--->| f(e_t + W_f x h_{t-1}^f + b_f) |--> h_t^f --'                                 (6)\n",
    "                                                                    +--------------------------------+\n",
    "                                                                                                   (5)\n",
    "```\n",
    "$W_w$, $W_e$, $W_b$, $W_f$ and $W_d$ are learned.  \n",
    "$b_e$, $b_b$, $b_f$ and $b_d$ are learned.  \n",
    "Figure of the whole pipeline (fig. 3?)\n",
    "##### 1.3 alignment objective.\n",
    "$S_{kl} = \\sum_{t \\in g_l}\\sum_{i \\in g_k}\\max(0, v_i^T s_t)$ (7)  \n",
    "$t \\in g_l$ is a sentence fragment in sentence $l$.  \n",
    "$i \\in g_l$ is an image fragment in image $k$.  \n",
    "--> similarity when vectors are positively aligned.\n",
    "\n",
    "$S_{kl} = \\sum_{t \\in g_l}\\max_{i \\in g_k}(v_i^T s_t)$ (8)  \n",
    "\"every word $s_t$ aligns to the single best image region.\"\n",
    "\n",
    "- the max-margin, structured loss:\n",
    "$C(\\theta) = \\sum_k\\left[\\sum_l\\max(0, S_{kl} - S_+{kk} + 1) + \\sum_l\\max(0, S_{lk} - S_+{kk} + 1)\\right]$ (9)  \n",
    "sum of rank images + rank sentences.\n",
    "##### 1.4 decoding text segment alignments to images.\n",
    "- generating snippets of text instead of single words.\n",
    "define a Markov Random Field:\n",
    "- sentence with N words.\n",
    "- image with M bounding boxes.\n",
    "  - $\\forall j \\in [1, N], a_j \\in [1, M]$\n",
    "  - $E(a) = \\sum_{j=1}^N\\psi_j^U(a_j) + \\sum_{j=1}^{N-1}\\psi_j^B(a_j, a_{j+1})$ (10)\n",
    "  - $\\psi_j^U(a_j) = v_i^T s_t$ (11)\n",
    "  - $\\psi_j^B(a_j, a_{j+1}) = \\beta 1[a_j = a_{j+1}]$ (12)\n",
    "### 2. TODO\n",
    "\n",
    "# II- The code\n",
    "##### 1. Basic torch initialization.\n",
    "##### 2. Create a data loader instance.\n",
    "##### 3. Initialize the networks\n",
    "- from file\n",
    "- from scratch:\n",
    "  - (1). the language model (`protos.lm`)\n",
    "  - (2). the convolutional network (`protos.cnn`)\n",
    "  - (3). the feature expander (`protos.expander`)\n",
    "  - (4). the language model criterion (`protos.crit`)\n",
    "  - use clone network to be able to write smaller checkpoints.\n",
    "##### 4. Validation evaluation (`eval_split`)\n",
    "- (1). fetch a batch of data, pre-process it, do not augment.\n",
    "- (2). forward pass :\n",
    "```\n",
    "         +-----+            +----------+                      +----+               +------+\n",
    "images --| cnn |--> feats --| expander |--> expanded_feats -,-| lm |--> logprobs --| crit |--> loss\n",
    "         +-----+            +----------+                   /  +----+               +------+\n",
    "                                                 labels --'\n",
    "```\n",
    "- (3). sample generation samples for each image.\n",
    "- (4). return `loss_sum / loss_evals, predictions={(id, caption)}. lang_stats`\n",
    "##### 5. Loss function (`lossFun`)\n",
    "- (1). forward pass to transform images into \"*back-propagatable*\" losses.\n",
    "```\n",
    "         +--------+\n",
    "images --| protos |--> loss\n",
    "         +--------+\n",
    "```\n",
    "- (2). backward pass: criterion, `lm` and `cnn` only if finetuning.\n",
    "- (3). clip gradients.\n",
    "- (4). apply L2 regularization.\n",
    "##### 6. Main loop\n",
    "- (1). eval loss and gradients.\n",
    "- (2). save checkpoints: opt, iter, loss_history, val_predictions.\n",
    "- (3). decay learning rates for `lm` and `cnn` $`\\epsilon =2^{-frac{i - i_0}{T}}`$\n",
    "- (4). parameters update.\n",
    "- (5). update `cnn` if not finetuning nor warming up.\n",
    "- (6). exploding loss or max iterations -> stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
