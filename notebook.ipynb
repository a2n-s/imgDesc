{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation of a field of Machine Learning (ML) research for the SD class at ISAE-Supaero (60 min) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   1.     Introduction (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1.   A brief overview on image captioning\n",
    "  The topic of captioning images is both a technical challenge and a crucial field of research in machine learning. Being able to automatically caption images in a natural language fashion would be very beneficial to the branch of machine learning interested in computer vision. This would not require the intervention of humans to write descriptive sentences about images that one want to use in a supervised dataset.\n",
    "  \n",
    "  It is also a challenge of great difficulty. For us humans, it is quite easy, almost trivial to look at an image and describe every part of it, in no time. For machine to do that automatically, it is quite the opposite. Some papers and algorithms have achieved some good results but mostly rely on hard-coded knowledge given by researchers and engineers.\n",
    "\n",
    "  We want to go beyond hard-coded visual concepts and sentence templates and generate, from any unseen image, a variable-sized, natural language sentence description.\n",
    "\n",
    "The idea is to do the following.  \n",
    "<img src=\"res/figure-1.png\" height=500>\n",
    "\n",
    "The algorithm takes as input an image, free of any caption and bounding boxes.  \n",
    "The network should then be able, after learning inferences on data only, i.e. without relying on any hard-coded knowledge, to cut the image into meaningful regions, caption them and finally construct a general caption for the whole image.\n",
    "\n",
    "This is what we see in the [Figure 1](res/figure-1.png). The original image is the one of a table covered with multiple objects. Regions are to be seen all over the scene, with their respective captions. And finally, a special region, the whole image in purple, is also captioned.\n",
    "\n",
    "In this document, the focus is put on a very old 2015 paper\n",
    "[Deep Visual-Semantic Alignments for Generating Image Descriptions][karpathy2015deep] from Andrej Karpathy and Li Fei-Fei.\n",
    "\n",
    "[karpathy2015deep-portal]:   https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=Deep+Visual-Semantic+Alignments+for+Generating+Image+Descriptions&btnG= \n",
    "[karpathy2015deep]:          https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf \n",
    "[karpathy2015deep-blog]:     https://cs.stanford.edu/people/karpathy/deepimagesent/ \n",
    "[karpathy2015deep-demo]:     https://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/\n",
    "[karpathy2015deep-code]:     https://github.com/karpathy/neuraltalk2\n",
    "[karpathy2015deep-codedep]:  https://github.com/karpathy/neuraltalk\n",
    "[karpathy2015deep-tmpvideo]: https://youtu.be/e-WB4lfg30M\n",
    "[karpathy2015deep-python]:   https://github.com/ruotianluo/neuraltalk2-tensorflow\n",
    "[karpathy2015deep-docker]:   https://github.com/SaMnCo/docker-neuraltalk2\n",
    "[coco]:                      https://cocodataset.org/#explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2.   The notebook\n",
    "This notebook is a work in the Decision Making & Data Science (SDD)\n",
    "class at ISAE-Supaero. The goal in to explain to other students,\n",
    "professors and more generally any person having a basic background\n",
    "in Machine Learning (ML), through a notebook designed to be played\n",
    "in around an hour, a complex enough ML subject.\n",
    "\n",
    "This notebook is organised as follows:\n",
    "- a dive into the theory of the paper.\n",
    "- more implementation details.\n",
    "- a focus on the results and online tools to caption images.\n",
    "\n",
    "Throughout the cells, one will find some external links to external\n",
    "resources, mainly code snippets or blog posts putting results forward.  \n",
    "\n",
    "This notebook has been timed with the help of people with basic\n",
    "algorithmic and ML knowledge but not ML-experts, thus some time\n",
    "anchors are being given to the reader as indication only.\n",
    "\n",
    "Finally this notebook has been run and designed under Google Colab and in Mozilla Firefox.\n",
    "Cells, images and inline monospace blocks might not appear right outside of Google Colab, e.g. it is not as pretty in Jupyter. Links might not open or work properly outside Firefox, e.g. the official web demo of the model introduced in this notebook for image captioning does not appear to work on Google Chrome.\n",
    "\n",
    "Moreover, this document goes along a directory of images, called `res`, with images ranging from `res/figure-1.png` to `res/figure-7.png`. Without this directory, given alongside the `notebook.ipynb` file, images probably won't display at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3.   Some resources\n",
    "One can find the list of the major resources used to write this notebook below:\n",
    "- the [scholar portal][karpathy2015deep-portal] and the [paper][karpathy2015deep].\n",
    "- the [standford post][karpathy2015deep-blog].\n",
    "- the [code][karpathy2015deep-code].\n",
    "- a [docker][karpathy2015deep-docker] wrap up.\n",
    "- a [python][karpathy2015deep-python] implementation.\n",
    "- a short (3 min) [explanatory video][karpathy2015deep-tmpvideo].\n",
    "\n",
    "[karpathy2015deep-portal]:   https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=Deep+Visual-Semantic+Alignments+for+Generating+Image+Descriptions&btnG= \n",
    "[karpathy2015deep]:          https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf \n",
    "[karpathy2015deep-blog]:     https://cs.stanford.edu/people/karpathy/deepimagesent/ \n",
    "[karpathy2015deep-demo]:     https://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/\n",
    "[karpathy2015deep-code]:     https://github.com/karpathy/neuraltalk2\n",
    "[karpathy2015deep-codedep]:  https://github.com/karpathy/neuraltalk\n",
    "[karpathy2015deep-tmpvideo]: https://youtu.be/e-WB4lfg30M\n",
    "[karpathy2015deep-python]:   https://github.com/ruotianluo/neuraltalk2-tensorflow\n",
    "[karpathy2015deep-docker]:   https://github.com/SaMnCo/docker-neuraltalk2\n",
    "[coco]:                      https://cocodataset.org/#explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4.   A few disclaimers\n",
    "Before diving into the paper and image captioning, some disclaimers\n",
    "have to be maid for the reader not to be disappointed nor surprised.\n",
    "\n",
    "None of the codes listed above appears to work nowadays. Compilation\n",
    "issues arise with the original `lua` code both on local machine and\n",
    "on Google Colab servers. The docker container does not produce any\n",
    "results which is not very helpful. And finally the most recent `python`\n",
    "implementation is old enough to be written in `python` 2.7 for which\n",
    "there is no more support and libraries do not install very well.\n",
    "\n",
    "Thus, the \"*implementation details*\" section is simply a collection of code snippets that the reader won't be able to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   2.     The paper (25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1.   Introduction (05 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the approach \"*is to infer [the image regions and sentence fragments] alignments and use them to learn a generative model of descriptions*\". The model is then composed of two sub models that solve one part of the overall problem:\n",
    "\n",
    "- \"*a deep neural network model that infers the latent alignment between segments of sentences and the region of the image that they describe*\". The latent space is the same for the image regions embeddings and the sentence fragments, as it will prove to be very convenient later.\n",
    "- \"*a multimodal Recurrent Neural Network\n",
    "  architecture that takes an input image and generates\n",
    "  its description in text*\".\n",
    "\n",
    "The full model shows state-of-the-art perfomances on the image captioning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related Work\n",
    "The four next paragraphs comes directly from the paper of *Karpathy et al.* as they already caption the main issues and solutions found in the field. They have been greatly shortened and simplified but the general ideas have been hopefully kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense image annotations**.\n",
    "\n",
    "Previous works have mainly focused on annotating images with correct labels usng fixed sets of categories, which is not the purpose of the paper which aims at a \"*richer and higher-level description of regions\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating descriptions**.  \n",
    "\n",
    "Many works focused on the retrieval aspect of image captioning. i.e. choosing the best description from a set of descriptions for a given input image or presented much more complex models. Here the goal is to generate variable-sized descriptions and the model is much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grounding natural language in images**.\n",
    "\n",
    "The authors take one step forward the idea of aligning contiguous fragments of sentences and contiguous image regions, which makes the model *\"more meaningful, interpretable\"* and the generation *\"not fized in length\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks in visual and language domains**.  \n",
    "\n",
    "The authors use Convolutional Neural Networks (CNN) and pretrained word vectors models to represent words and moreover condition the model on images to gain more context for the sentence generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2.   Learning to align visual and language data (15 min)\n",
    "As stated above, we suppose in the following that we have access to huge datasets\n",
    "composed of image-sentence pairs, i.e. an image and sentences describing the\n",
    "elements in the image, in a natural language format.\n",
    "\n",
    "For instance, the authors use the Flickr8K, Flickr30K and MSCOCO datasets which are\n",
    "composed of exactly these pairs. I really do recommend playing with the\n",
    "[online image browser][coco] of Microsoft. It is a fun and powerful way to look at\n",
    "the dataset. We see that the items in the set are really only images and their caption\n",
    "sentences describing the objects.\n",
    "\n",
    "Once we have the dataset and the problem -as a reminder, we would like to\n",
    "generate new captions for unseen images by using not only fixed-size descriptions\n",
    "allowing richer captioning- we need to sketch the overall pipeline of the future\n",
    "algorithm. This is summarized in the [Figure 2](res/figure-2.png) below.  \n",
    "<img src=\"res/figure-2.png\" height=500>\n",
    "\n",
    "The key insight of this pipeline is that, according to the authors, \"people make\n",
    "frequent references to some particular, but unknown location in the image.\" Thus\n",
    "one algorithm first need to infer from full sentences what part of the sentence\n",
    "are related to what part of the image, this is the \"inferred correspondences\".\n",
    "Once the algorithm has internalized this knowledge, we want it to put back snippets\n",
    "of captioning for sub-images to form a general description of the image.\n",
    "\n",
    "This first section focuses on the first  half of the process, i.e. going from whole\n",
    "sentence descriptions to inferred correspondences.\n",
    "\n",
    "[coco]:                      https://cocodataset.org/#explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Representing images (05 min)\n",
    "First of all, we need to represent the images in some latent space.\n",
    "\n",
    "*Reminder: a latent\n",
    "space is generally a space of lower dimension containing a small amount of but\n",
    "hopefully enough information about the original data. Also referred to as an\n",
    "embedded space.*\n",
    "\n",
    "The idea is to map images from their very-high dimensional native spaces of images to\n",
    "20 $h$-dimensional vectors, ${v_i | i = 1, ..., 20}$. To achieve this embedding, we\n",
    "use a pre-trained Region Convolutional Neural Network (RCNN) trained on the ImageNet\n",
    "dataset and finetuned on the top 200 classes of the ImageNet Detection Challenge.\n",
    "\n",
    "Then, to construct the latent vectors, the authors used the top 19 classification regions\n",
    "plus an extra region for the whole image and the formula  \n",
    "$$\\forall i \\in [|1, 20|], v_i = W_m[CNN_{\\theta}(I_{b_i})] + b_m (1)$$\n",
    "Where:\n",
    "  - $W_m$ is a $h \\times 4096$-dimensional learned weight matrix\n",
    "  - $CNN_{\\theta}$ is a mapping from bounding boxes pixels in the image to $4096$-dimensional vectors.\n",
    "  - $\\theta$ has 60 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "     +-----------------+\n",
    "     | B               |\n",
    "   +-----------------+ |                     ,----------.   +------------------------+     +---------+    ,------.\n",
    "   | G               | |                     (  I_{b_1} )-->|  CNN_{\\theta}(I_{b_1}) |---->| W_m * . |--> (  v_1 )\n",
    " +-----------------+ | |                     (          )   +------------------------+     +---------+    (      )\n",
    " | R               | | |       +------+      (    .     )                .                      .         (  .   )\n",
    " |                 | | | ----> | RCNN | ---> (    .     )                .                      .         (  .   )\n",
    " |                 | | |       +------+      (    .     )                .                      .         (  .   )\n",
    " |      INPUT      | | |                     (          )   +------------------------+     +---------+    (      )\n",
    " |      IMAGE      | |-+                     ( I_{b_20} )-->| CNN_{\\theta}(I_{b_20}) |---->| W_m * . |--> ( v_20 )\n",
    " |                 | |                       `----------'   +------------------------+     +---------+    `------'\n",
    " |                 |-+\n",
    " |                 |\n",
    " +-----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Representing sentences (05 min)\n",
    "On the other hand, we need to represent the sentences as well. The choice has\n",
    "been made to encode them in the same $h$-dimensional embedding space as the images.\n",
    "\n",
    "This choice has the advantage we will be able to compare images and sentences, define\n",
    "metrics between them, e.g. with the dot product as we will see later in the notebook.\n",
    "\n",
    "To explain this part, I created the following graph showing the flow of information\n",
    "across the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        +-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "        |                                                                                                                                                     |\n",
    "        |                                                                +--------------------------------+                                                   |\n",
    "        |                                                           .--->| f(e_t + W_b x h_{t+1}^b + b_b) |--> h_t^b --.                                      |\n",
    "        |  +-----------+          +--------------------+           /     +--------------------------------+             \\    +-----------------------------+  |\n",
    "1_t --->|--| W_m * 1_t |--> x_t --| f(W_e * x_t + b_e) |--> e_t --*                                     (4)              *-->| f(W_d(h_t^f + h_t^b) + b_d) |--|---> s_t\n",
    "        |  +-----------+          +--------------------+           \\     +--------------------------------+             /    +-----------------------------+  |\n",
    "        |            (2)                             (3)            `--->| f(e_t + W_f x h_{t-1}^f + b_f) |--> h_t^f --'                                 (6)  |\n",
    "        |                                                                +--------------------------------+                                                   |\n",
    "        |------+                                                                                        (5)                                                   |\n",
    "        | BRNN |                                                                                                                                              |\n",
    "        +-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes about the above network flow:\n",
    "- the whole network is called a Bidirectional Recurrent Neural Network (BRNN) which takes 1-hot encoding of N words over an alphabet as inputs and spits out $h$-dimensional partial score vector.\n",
    "- $W_w$, $W_e$, $W_b$, $W_f$ and $W_d$ are learned weights.\n",
    "- $b_e$, $b_b$, $b_f$ and $b_d$ are learned biases.\n",
    "- $f$ is the ReLu activation function which is defined, from $\\mathbb{R}$ to $\\mathbb{R}_{+}$, as $f: x \\mapsto \\max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Alignment objective (05 min)\n",
    "Now that we are able to learn representations for both the input images and their corresponding description\n",
    "sentences, we need to define a way to pair them and to give them scores. This is done as in following [Figure 3](res/figure-3.png),\n",
    "in which one will find the two previous architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/figure-3.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, the image representations are generated thanks to the RCNN onto a latent space.  \n",
    "On the right, the sentence representations are generated as well thanks to the BRNN onto the same latent space.  \n",
    "Once all of this is computed, we are able to compute the pair-wise scores, i.e. the gray-scale matrix in the middle,\n",
    "and then aggregate these scores into the top vector. This can summarized in the below equation:\n",
    "- $S_{kl} = \\sum_{t \\in g_l}\\sum_{i \\in g_k}\\max(0, v_i^T s_t)$ (7)\n",
    "  - where $t \\in g_l$ is a sentence fragment in sentence $l$.\n",
    "  - and $i \\in g_l$ is an image fragment in image $k$.\n",
    "  - we say that the is a similarity when vectors are positively aligned. That is when the words have a confident support in the image.\n",
    "- and the simpler equivalent form: $S_{kl} = \\sum_{t \\in g_l}\\max_{i \\in g_k}(v_i^T s_t)$ (8) which makes sure that \"every word $s_t$ aligns to the single best image region.\"\n",
    "\n",
    "Finally, the max-margin, structured loss is defined as  \n",
    "$$C(\\theta) = \\sum_k\\left[\\sum_l\\max(0, S_{kl} - S_+{kk} + 1) + \\sum_l\\max(0, S_{lk} - S_+{kk} + 1)\\right] (9) $$  \n",
    "It is the general loss used as a learning criterion and can be seen as the sum of rank images plus the sum of rank sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Decoding text segment alignments to images (optional)\n",
    "At this point, the algorithm should be able to associate image regions and sentence words. However,\n",
    "if only done naively, we would only obtain words scattered inconsistently. This is not the goal\n",
    "set at the beginning! We want the algorithm to generate complete sentence descriptions to caption\n",
    "unseen images.\n",
    "\n",
    "The following allows the network to generate snippets of text instead of single words.\n",
    "\n",
    "To solve this, the authors see the problem as a Markov Random Field with:\n",
    "- a sentence with N words.\n",
    "- an image with M bounding boxes.\n",
    "- the following equations:\n",
    "  - $\\forall j \\in [1, N], a_j \\in [1, M]$\n",
    "  - $E(a) = \\sum_{j=1}^N\\psi_j^U(a_j) + \\sum_{j=1}^{N-1}\\psi_j^B(a_j, a_{j+1})$ (10)\n",
    "  - $\\psi_j^U(a_j) = v_i^T s_t$ (11)\n",
    "  - $\\psi_j^B(a_j, a_{j+1}) = \\beta 1[a_j = a_{j+1}]$ (12)\n",
    "\n",
    "The $\\beta$ hyperparameter represents the \"length\" of the final sentence, i.e. $\\beta = 0$\n",
    "corresponds to single-word alignment, whereas increasing $\\beta$ leads to longer sentences.  \n",
    "Finally, we minimize the energy $E$ using dynamic programming. The result of this solving\n",
    "process is then a set of images regions annotated with segments of text. Recall [Figure 1](res/figure-1.png)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/figure-1.png\" height=500>  \n",
    "This is exactly the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3.   Multimodal Recurrent Neural Network for generating descriptions. (05 min)\n",
    "In the previous section, we described a neural network pipeline and data flow to align\n",
    "image regions and description snippets from a given dataset, namely one of the Flickr\n",
    "or COCO.\n",
    "\n",
    "In this section, we take this knowledge further and try to generate brand new snippets\n",
    "of description for new images, i.e. we want to predict variable-sized sequences of words\n",
    "that properly caption the elements in the image.\n",
    "\n",
    "The idea is then to use a Recurrent Neural Network (RNN) that will predict the probability\n",
    "distribution of the next word given the previous one and the context around the current word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/figure-4.png\" height=500>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Multimodal RNN (MRNN)!  \n",
    "Let us dive into the formal details of the MRNN.  \n",
    "First we build the context thanks to the input image, which obviously represents the context as\n",
    "it contains all the objects that the network should caption, and the CNN from previous sections.\n",
    "The context is then defined as $b_v = W_{h_i}[CNN_{\\theta_c}(I)]$ with $I$ being the pixels of\n",
    "the image and $CNN_{\\theta_c}$ the last layer of the CNN.  \n",
    "We then compute the unnormalized log probabilities of the words in the dictionary, called $y_t$,\n",
    "as $y_t = softmax(W_{oh}h_t + b_o)$ where $h_t$ is the hidden state of the MRNN and defined as\n",
    "$h_t = f(W_{hx}x_t + W_{hh}h_{t-1} + b_h + b_v)$.  All the weights and biases are here learned.  \n",
    "*Note: the authors found that given the context $b_v$ only at the first time step yields better\n",
    "results. This results in the diagram from [Figure 4](res/figure-4.png).*\n",
    "\n",
    "At test time, one computes the context $b_v$, sets $h_0$ to $0$ and $x_1$ to the special *START*\n",
    "token. Then the idea is simply to either sample from the distributions predicted by the MRNN, or\n",
    "take the argmax over the probabilities, and repeat the process until the special *END* token is\n",
    "predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4.   Optimization. (optional)\n",
    "In this short section are listed some of the key technical details about the learning phases:\n",
    "- the alignment problem:\n",
    "  - SGD as the optimizer\n",
    "  - mini-batches of size 100\n",
    "  - a momentum for SGD of 0.9\n",
    "  - the use of learning rate and weight decay cross-validation\n",
    "  - dropout layers except for recurrent ones\n",
    "  - gradient clipping at 5 to keep their magnitudes low enough\n",
    "- the generation problem:\n",
    "  - the optimizer is RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   3.     Some implementation details. (10 min)\n",
    "As stated at the beginning of the cells, I did not manage to get any of the codes I found running.  \n",
    "The next 6 sub-sections are not meant to be fully read or detailed. They are here to show a high overview of the ML pipeline allowing the results of image captioning that we will look at closely in last section.\n",
    "\n",
    "The code comes from the second and final version of the original code that goes with the paper of *Karpathy et al.*  \n",
    "It uses `torch` and is written in `lua`, which is a programming language that has some similarities with `python` in the sense that it should be as easy to read as `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.   Basic torch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local opt = cmd:parse(arg)\n",
    "torch.manualSeed(opt.seed)\n",
    "torch.setdefaulttensortype('torch.FloatTensor') -- for CPU\n",
    "\n",
    "if opt.gpuid >= 0 then\n",
    "  require 'cutorch'\n",
    "  require 'cunn'\n",
    "  if opt.backend == 'cudnn' then require 'cudnn' end\n",
    "  cutorch.manualSeed(opt.seed)\n",
    "  cutorch.setDevice(opt.gpuid + 1) -- note +1 because lua is 1-indexed\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2.   Create a data loader instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local loader = DataLoader{h5_file = opt.input_h5, json_file = opt.input_json}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.3.   Initialize the networks\n",
    "- from file\n",
    "- from scratch:\n",
    "  - (1). the language model (`protos.lm`)\n",
    "  - (2). the convolutional network (`protos.cnn`)\n",
    "  - (3). the feature expander (`protos.expander`)\n",
    "  - (4). the language model criterion (`protos.crit`)\n",
    "  - use clone network to be able to write smaller checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local protos = {}\n",
    "\n",
    "if string.len(opt.start_from) > 0 then\n",
    "  -- load protos from file\n",
    "  print('initializing weights from ' .. opt.start_from)\n",
    "  local loaded_checkpoint = torch.load(opt.start_from)\n",
    "  protos = loaded_checkpoint.protos\n",
    "  net_utils.unsanitize_gradients(protos.cnn)\n",
    "  local lm_modules = protos.lm:getModulesList()\n",
    "  for k,v in pairs(lm_modules) do net_utils.unsanitize_gradients(v) end\n",
    "  protos.crit = nn.LanguageModelCriterion() -- not in checkpoints, create manually\n",
    "  protos.expander = nn.FeatExpander(opt.seq_per_img) -- not in checkpoints, create manually\n",
    "else\n",
    "  -- create protos from scratch\n",
    "  -- intialize language model\n",
    "  local lmOpt = {}\n",
    "  lmOpt.vocab_size = loader:getVocabSize()\n",
    "  lmOpt.input_encoding_size = opt.input_encoding_size\n",
    "  lmOpt.rnn_size = opt.rnn_size\n",
    "  lmOpt.num_layers = 1\n",
    "  lmOpt.dropout = opt.drop_prob_lm\n",
    "  lmOpt.seq_length = loader:getSeqLength()\n",
    "  lmOpt.batch_size = opt.batch_size * opt.seq_per_img\n",
    "  protos.lm = nn.LanguageModel(lmOpt)\n",
    "  -- initialize the ConvNet\n",
    "  local cnn_backend = opt.backend\n",
    "  if opt.gpuid == -1 then cnn_backend = 'nn' end -- override to nn if gpu is disabled\n",
    "  local cnn_raw = loadcaffe.load(opt.cnn_proto, opt.cnn_model, cnn_backend)\n",
    "  protos.cnn = net_utils.build_cnn(cnn_raw, {encoding_size = opt.input_encoding_size, backend = cnn_backend})\n",
    "  -- initialize a special FeatExpander module that \"corrects\" for the batch number discrepancy \n",
    "  -- where we have multiple captions per one image in a batch. This is done for efficiency\n",
    "  -- because doing a CNN forward pass is expensive. We expand out the CNN features for each sentence\n",
    "  protos.expander = nn.FeatExpander(opt.seq_per_img)\n",
    "  -- criterion for the language model\n",
    "  protos.crit = nn.LanguageModelCriterion()\n",
    "end\n",
    "\n",
    "-- ship everything to GPU, maybe\n",
    "if opt.gpuid >= 0 then\n",
    "  for k,v in pairs(protos) do v:cuda() end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "-- flatten and prepare all model parameters to a single vector. \n",
    "-- Keep CNN params separate in case we want to try to get fancy with different optims on LM/CNN\n",
    "local params, grad_params = protos.lm:getParameters()\n",
    "local cnn_params, cnn_grad_params = protos.cnn:getParameters()\n",
    "print('total number of parameters in LM: ', params:nElement())\n",
    "print('total number of parameters in CNN: ', cnn_params:nElement())\n",
    "assert(params:nElement() == grad_params:nElement())\n",
    "assert(cnn_params:nElement() == cnn_grad_params:nElement())\n",
    "\n",
    "-- construct thin module clones that share parameters with the actual\n",
    "-- modules. These thin module will have no intermediates and will be used\n",
    "-- for checkpointing to write significantly smaller checkpoint files\n",
    "local thin_lm = protos.lm:clone()\n",
    "thin_lm.core:share(protos.lm.core, 'weight', 'bias') -- TODO: we are assuming that LM has specific members! figure out clean way to get rid of, not modular.\n",
    "thin_lm.lookup_table:share(protos.lm.lookup_table, 'weight', 'bias')\n",
    "local thin_cnn = protos.cnn:clone('weight', 'bias')\n",
    "-- sanitize all modules of gradient storage so that we dont save big checkpoints\n",
    "net_utils.sanitize_gradients(thin_cnn)\n",
    "local lm_modules = thin_lm:getModulesList()\n",
    "for k,v in pairs(lm_modules) do net_utils.sanitize_gradients(v) end\n",
    "\n",
    "-- create clones and ensure parameter sharing. we have to do this \n",
    "-- all the way here at the end because calls such as :cuda() and\n",
    "-- :getParameters() reshuffle memory around.\n",
    "protos.lm:createClones()\n",
    "\n",
    "collectgarbage() -- \"yeah, sure why not\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.4.   Validation evaluation (`eval_split`)\n",
    "- (1). fetch a batch of data, pre-process it, do not augment.\n",
    "- (2). forward pass :\n",
    "```\n",
    "         +-----+            +----------+                      +----+               +------+\n",
    "images --| cnn |--> feats --| expander |--> expanded_feats -,-| lm |--> logprobs --| crit |--> loss\n",
    "         +-----+            +----------+                   /  +----+               +------+\n",
    "                                                 labels --'\n",
    "```\n",
    "- (3). sample generation samples for each image.\n",
    "- (4). return `loss_sum / loss_evals, predictions={(id, caption)}. lang_stats`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local function eval_split(split, evalopt)\n",
    "  local verbose = utils.getopt(evalopt, 'verbose', true)\n",
    "  local val_images_use = utils.getopt(evalopt, 'val_images_use', true)\n",
    "\n",
    "  protos.cnn:evaluate()\n",
    "  protos.lm:evaluate()\n",
    "  loader:resetIterator(split) -- rewind iteator back to first datapoint in the split\n",
    "  local n = 0\n",
    "  local loss_sum = 0\n",
    "  local loss_evals = 0\n",
    "  local predictions = {}\n",
    "  local vocab = loader:getVocab()\n",
    "  while true do\n",
    "\n",
    "    -- fetch a batch of data\n",
    "    local data = loader:getBatch{batch_size = opt.batch_size, split = split, seq_per_img = opt.seq_per_img}\n",
    "    data.images = net_utils.prepro(data.images, false, opt.gpuid >= 0) -- preprocess in place, and don't augment\n",
    "    n = n + data.images:size(1)\n",
    "\n",
    "    -- forward the model to get loss\n",
    "    local feats = protos.cnn:forward(data.images)\n",
    "    local expanded_feats = protos.expander:forward(feats)\n",
    "    local logprobs = protos.lm:forward{expanded_feats, data.labels}\n",
    "    local loss = protos.crit:forward(logprobs, data.labels)\n",
    "    loss_sum = loss_sum + loss\n",
    "    loss_evals = loss_evals + 1\n",
    "\n",
    "    -- forward the model to also get generated samples for each image\n",
    "    local seq = protos.lm:sample(feats)\n",
    "    local sents = net_utils.decode_sequence(vocab, seq)\n",
    "    for k=1,#sents do\n",
    "      local entry = {image_id = data.infos[k].id, caption = sents[k]}\n",
    "      table.insert(predictions, entry)\n",
    "      if verbose then\n",
    "        print(string.format('image %s: %s', entry.image_id, entry.caption))\n",
    "      end\n",
    "    end\n",
    "\n",
    "    -- if we wrapped around the split or used up val imgs budget then bail\n",
    "    local ix0 = data.bounds.it_pos_now\n",
    "    local ix1 = math.min(data.bounds.it_max, val_images_use)\n",
    "    if verbose then\n",
    "      print(string.format('evaluating validation performance... %d/%d (%f)', ix0-1, ix1, loss))\n",
    "    end\n",
    "\n",
    "    if loss_evals % 10 == 0 then collectgarbage() end\n",
    "    if data.bounds.wrapped then break end -- the split ran out of data, lets break out\n",
    "    if n >= val_images_use then break end -- we've used enough images\n",
    "  end\n",
    "\n",
    "  local lang_stats\n",
    "  if opt.language_eval == 1 then\n",
    "    lang_stats = net_utils.language_eval(predictions, opt.id)\n",
    "  end\n",
    "\n",
    "  return loss_sum/loss_evals, predictions, lang_stats\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.5.   Loss function (`lossFun`)\n",
    "- (1). forward pass to transform images into losses that one can back-propagate through.\n",
    "```\n",
    "         +--------+\n",
    "images --| protos |--> loss\n",
    "         +--------+\n",
    "```\n",
    "- (2). backward pass: criterion, `lm` and `cnn` only if fine-tuning.\n",
    "- (3). clip gradients.\n",
    "- (4). apply L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local iter = 0\n",
    "local function lossFun()\n",
    "  protos.cnn:training()\n",
    "  protos.lm:training()\n",
    "  grad_params:zero()\n",
    "  if opt.finetune_cnn_after >= 0 and iter >= opt.finetune_cnn_after then\n",
    "    cnn_grad_params:zero()\n",
    "  end\n",
    "\n",
    "  -----------------------------------------------------------------------------\n",
    "  -- Forward pass\n",
    "  -----------------------------------------------------------------------------\n",
    "  -- get batch of data  \n",
    "  local data = loader:getBatch{batch_size = opt.batch_size, split = 'train', seq_per_img = opt.seq_per_img}\n",
    "  data.images = net_utils.prepro(data.images, true, opt.gpuid >= 0) -- preprocess in place, do data augmentation\n",
    "  -- data.images: Nx3x224x224 \n",
    "  -- data.seq: LxM where L is sequence length upper bound, and M = N*seq_per_img\n",
    "\n",
    "  -- forward the ConvNet on images (most work happens here)\n",
    "  local feats = protos.cnn:forward(data.images)\n",
    "  -- we have to expand out image features, once for each sentence\n",
    "  local expanded_feats = protos.expander:forward(feats)\n",
    "  -- forward the language model\n",
    "  local logprobs = protos.lm:forward{expanded_feats, data.labels}\n",
    "  -- forward the language model criterion\n",
    "  local loss = protos.crit:forward(logprobs, data.labels)\n",
    "  \n",
    "  -----------------------------------------------------------------------------\n",
    "  -- Backward pass\n",
    "  -----------------------------------------------------------------------------\n",
    "  -- backprop criterion\n",
    "  local dlogprobs = protos.crit:backward(logprobs, data.labels)\n",
    "  -- backprop language model\n",
    "  local dexpanded_feats, ddummy = unpack(protos.lm:backward({expanded_feats, data.labels}, dlogprobs))\n",
    "  -- backprop the CNN, but only if we are finetuning\n",
    "  if opt.finetune_cnn_after >= 0 and iter >= opt.finetune_cnn_after then\n",
    "    local dfeats = protos.expander:backward(feats, dexpanded_feats)\n",
    "    local dx = protos.cnn:backward(data.images, dfeats)\n",
    "  end\n",
    "\n",
    "  -- clip gradients\n",
    "  -- print(string.format('claming %f%% of gradients', 100*torch.mean(torch.gt(torch.abs(grad_params), opt.grad_clip))))\n",
    "  grad_params:clamp(-opt.grad_clip, opt.grad_clip)\n",
    "\n",
    "  -- apply L2 regularization\n",
    "  if opt.cnn_weight_decay > 0 then\n",
    "    cnn_grad_params:add(opt.cnn_weight_decay, cnn_params)\n",
    "    -- note: we don't bother adding the l2 loss to the total loss, meh.\n",
    "    cnn_grad_params:clamp(-opt.grad_clip, opt.grad_clip)\n",
    "  end\n",
    "  -----------------------------------------------------------------------------\n",
    "\n",
    "  -- and lets get out!\n",
    "  local losses = { total_loss = loss }\n",
    "  return losses\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.6.   Main loop\n",
    "- (1). evaluation loss and gradients.\n",
    "- (2). save checkpoints: opt, iteration, loss_history, val_predictions.\n",
    "- (3). decay learning rates for `lm` and `cnn` $\\epsilon =2^{-\\frac{i - i_0}{T}}$\n",
    "- (4). parameters update.\n",
    "- (5). update `cnn` if not fine-tuning nor warming up.\n",
    "- (6). exploding loss or max iterations -> stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lua\n",
    "local optim_state = {}\n",
    "local cnn_optim_state = {}\n",
    "local loss_history = {}\n",
    "local val_lang_stats_history = {}\n",
    "local val_loss_history = {}\n",
    "local best_score\n",
    "while true do  \n",
    "\n",
    "  -- eval loss/gradient\n",
    "  local losses = lossFun()\n",
    "  if iter % opt.losses_log_every == 0 then loss_history[iter] = losses.total_loss end\n",
    "  print(string.format('iter %d: %f', iter, losses.total_loss))\n",
    "\n",
    "  -- save checkpoint once in a while (or on final iteration)\n",
    "  if (iter % opt.save_checkpoint_every == 0 or iter == opt.max_iters) then\n",
    "\n",
    "    -- evaluate the validation performance\n",
    "    local val_loss, val_predictions, lang_stats = eval_split('val', {val_images_use = opt.val_images_use})\n",
    "    print('validation loss: ', val_loss)\n",
    "    print(lang_stats)\n",
    "    val_loss_history[iter] = val_loss\n",
    "    if lang_stats then\n",
    "      val_lang_stats_history[iter] = lang_stats\n",
    "    end\n",
    "\n",
    "    local checkpoint_path = path.join(opt.checkpoint_path, 'model_id' .. opt.id)\n",
    "\n",
    "    -- write a (thin) json report\n",
    "    local checkpoint = {}\n",
    "    checkpoint.opt = opt\n",
    "    checkpoint.iter = iter\n",
    "    checkpoint.loss_history = loss_history\n",
    "    checkpoint.val_loss_history = val_loss_history\n",
    "    checkpoint.val_predictions = val_predictions -- save these too for CIDEr/METEOR/etc eval\n",
    "    checkpoint.val_lang_stats_history = val_lang_stats_history\n",
    "\n",
    "    utils.write_json(checkpoint_path .. '.json', checkpoint)\n",
    "    print('wrote json checkpoint to ' .. checkpoint_path .. '.json')\n",
    "\n",
    "    -- write the full model checkpoint as well if we did better than ever\n",
    "    local current_score\n",
    "    if lang_stats then\n",
    "      -- use CIDEr score for deciding how well we did\n",
    "      current_score = lang_stats['CIDEr']\n",
    "    else\n",
    "      -- use the (negative) validation loss as a score\n",
    "      current_score = -val_loss\n",
    "    end\n",
    "    if best_score == nil or current_score > best_score then\n",
    "      best_score = current_score\n",
    "      if iter > 0 then -- dont save on very first iteration\n",
    "        -- include the protos (which have weights) and save to file\n",
    "        local save_protos = {}\n",
    "        save_protos.lm = thin_lm -- these are shared clones, and point to correct param storage\n",
    "        save_protos.cnn = thin_cnn\n",
    "        checkpoint.protos = save_protos\n",
    "        -- also include the vocabulary mapping so that we can use the checkpoint \n",
    "        -- alone to run on arbitrary images without the data loader\n",
    "        checkpoint.vocab = loader:getVocab()\n",
    "        torch.save(checkpoint_path .. '.t7', checkpoint)\n",
    "        print('wrote checkpoint to ' .. checkpoint_path .. '.t7')\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "\n",
    "  -- decay the learning rate for both LM and CNN\n",
    "  local learning_rate = opt.learning_rate\n",
    "  local cnn_learning_rate = opt.cnn_learning_rate\n",
    "  if iter > opt.learning_rate_decay_start and opt.learning_rate_decay_start >= 0 then\n",
    "    local frac = (iter - opt.learning_rate_decay_start) / opt.learning_rate_decay_every\n",
    "    local decay_factor = math.pow(0.5, frac)\n",
    "    learning_rate = learning_rate * decay_factor -- set the decayed rate\n",
    "    cnn_learning_rate = cnn_learning_rate * decay_factor\n",
    "  end\n",
    "\n",
    "  -- perform a parameter update\n",
    "  if opt.optim == 'rmsprop' then\n",
    "    rmsprop(params, grad_params, learning_rate, opt.optim_alpha, opt.optim_epsilon, optim_state)\n",
    "  elseif opt.optim == 'adagrad' then\n",
    "    adagrad(params, grad_params, learning_rate, opt.optim_epsilon, optim_state)\n",
    "  elseif opt.optim == 'sgd' then\n",
    "    sgd(params, grad_params, opt.learning_rate)\n",
    "  elseif opt.optim == 'sgdm' then\n",
    "    sgdm(params, grad_params, learning_rate, opt.optim_alpha, optim_state)\n",
    "  elseif opt.optim == 'sgdmom' then\n",
    "    sgdmom(params, grad_params, learning_rate, opt.optim_alpha, optim_state)\n",
    "  elseif opt.optim == 'adam' then\n",
    "    adam(params, grad_params, learning_rate, opt.optim_alpha, opt.optim_beta, opt.optim_epsilon, optim_state)\n",
    "  else\n",
    "    error('bad option opt.optim')\n",
    "  end\n",
    "\n",
    "  -- do a cnn update (if finetuning, and if rnn above us is not warming up right now)\n",
    "  if opt.finetune_cnn_after >= 0 and iter >= opt.finetune_cnn_after then\n",
    "    if opt.cnn_optim == 'sgd' then\n",
    "      sgd(cnn_params, cnn_grad_params, cnn_learning_rate)\n",
    "    elseif opt.cnn_optim == 'sgdm' then\n",
    "      sgdm(cnn_params, cnn_grad_params, cnn_learning_rate, opt.cnn_optim_alpha, cnn_optim_state)\n",
    "    elseif opt.cnn_optim == 'adam' then\n",
    "      adam(cnn_params, cnn_grad_params, cnn_learning_rate, opt.cnn_optim_alpha, opt.cnn_optim_beta, opt.optim_epsilon, cnn_optim_state)\n",
    "    else\n",
    "      error('bad option for opt.cnn_optim')\n",
    "    end\n",
    "  end\n",
    "\n",
    "  -- stopping criterions\n",
    "  iter = iter + 1\n",
    "  if iter % 10 == 0 then collectgarbage() end -- good idea to do this once in a while, i think\n",
    "  if loss0 == nil then loss0 = losses.total_loss end\n",
    "  if losses.total_loss > loss0 * 20 then\n",
    "    print('loss seems to be exploding, quitting.')\n",
    "    break\n",
    "  end\n",
    "  if opt.max_iters > 0 and iter >= opt.max_iters then break end -- stopping criterion\n",
    "\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   4.    Experiments and results. (15 min)\n",
    "The paper set new state-of-the-art performance results on the Flickr8K and Flickr30K\n",
    "datasets and proposes a new baseline for the MSCOCO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1.  The alignment. (03 min)\n",
    "The main takeaways:\n",
    "- the alignment/generation model outperforms previous works on image captioning.\n",
    "- the use of their simpler objective function increases captioning performances.\n",
    "- the network discovers interpretable visual-semantic correspondences\n",
    "  - <img src=\"res/figure-5.png\" height=500>\n",
    "- very discriminative words like \"kayak\" or \"pumpkin\" have high magnitudes in the\n",
    "laten space, whereas stop words such as  \"now\" or \"but\" are being projected near\n",
    "the origin of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2.  The fulframe generative model. (01 min)\n",
    "<img src=\"res/figure-6.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.3.  The region generative model. (01 min)\n",
    "<img src=\"res/figure-7.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.4.  Play with the generated results online. (10 min)\n",
    "To finish off this presentation, I encourage the reader to visit the [web demo][karpathy2015deep-demo]\n",
    "that goes alongside the paper.\n",
    "\n",
    "The reader may find there the same kind of image descriptions as in [Figure 6](res/figure-6.png), with scores and individual\n",
    "snippets breakdown as in [Figure 7](res/figure-7.png).\n",
    "\n",
    "[karpathy2015deep-demo]:     https://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
